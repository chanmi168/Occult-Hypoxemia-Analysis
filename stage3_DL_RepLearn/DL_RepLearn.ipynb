{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25793c03",
   "metadata": {},
   "source": [
    "# load data\n",
    "# design model\n",
    "# test model\n",
    "# TODO: continue to debug train_sweep block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import json\n",
    "import pytz\n",
    "import pprint\n",
    "\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# %pip install torch-summary\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # add this line so data are visible in this file\n",
    "sys.path.append('../../') # add this line so data are visible in this file\n",
    "sys.path.append('../PhysioMC/') # add this line so data are visible in this file\n",
    "\n",
    "# from PatchWand import *\n",
    "from filters import *\n",
    "from setting import *\n",
    "# from preprocessing import *\n",
    "from ECG_module import *\n",
    "from dataIO import *\n",
    "from evaluate import *\n",
    "from stage1_PPG_analysis import *\n",
    "from plotting_tools import *\n",
    "from stage4_regression import *\n",
    "\n",
    "\n",
    "from DR_extension.training_util import *\n",
    "from DR_extension.dataset_util import *\n",
    "from DR_extension.evaluation_util import *\n",
    "from DR_extension.models import *\n",
    "from DR_extension.models_CNNlight import *\n",
    "\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807d5b1f-2c4c-4f58-8e3a-d177a076188f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76124baf-3971-451b-b49f-66c4909ff768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa98f2f-2a31-41fc-8260-6257748b44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900e03c-4685-4a3d-bc4c-7c9e1cbdf2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='feature_learning')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "# args = parser.parse_args(['--input_folder', '../../data/stage1/waveform/', \n",
    "args = parser.parse_args(['--input_folder', '../../data/stage3_DL_prepare/', \n",
    "                          '--output_folder', '../../data/stage3_DL_RepLearn/',\n",
    "                          '--training_params_file', 'training_params_baseline.json',\n",
    "                          # '--training_params_file', 'training_params_dummy.json',\n",
    "                         ])\n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ed2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_id = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c93df87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1082f8d-abe1-4ed3-b4bd-7042f6111d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputdir = '../../data/stage1/waveform/'\n",
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "training_params_file = args.training_params_file\n",
    "\n",
    "# outputdir = '../../data/stage3_DL_RepLearn/'\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48618ab5-215c-4d70-a8d1-07621b8ece8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_names(training_params):\n",
    "    model_out_names = []\n",
    "\n",
    "#     for output_name in training_params['output_names']:\n",
    "    for output_name in training_params['output_names']:\n",
    "        for input_name in training_params['input_names']:\n",
    "            model_out_names.append(output_name+'-{}'.format(input_name))\n",
    "    return model_out_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a8edf-a34c-4438-b633-9d5548a970e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(label, training_params):\n",
    "\n",
    "    dataset_dict = training_params['dataset_dict']\n",
    "\n",
    "    race_encoder = LabelEncoder()\n",
    "    i_race = dataset_dict['list_label'].index('Race String')\n",
    "    label[:, i_race] = race_encoder.fit_transform(label[:, i_race])\n",
    "\n",
    "    PAT_ID_encoder = LabelEncoder()\n",
    "    i_PAT_ID = dataset_dict['list_label'].index('PAT_ID')\n",
    "    label[:, i_PAT_ID] = PAT_ID_encoder.fit_transform(label[:, i_PAT_ID])\n",
    "\n",
    "    split_name_encoder = LabelEncoder()\n",
    "    i_split_name = dataset_dict['list_label'].index('split_name')\n",
    "    label[:, i_split_name] = split_name_encoder.fit_transform(label[:, i_split_name])\n",
    "    \n",
    "    training_params['PAT_ID_encoder'] = PAT_ID_encoder\n",
    "    training_params['race_encoder'] = race_encoder\n",
    "    training_params['split_name_encoder'] = split_name_encoder\n",
    "\n",
    "    return label, training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d091bf9-6714-4e19-a9fc-60c8db3840c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_dann\n",
    "evaler = eval_dann\n",
    "preder = pred_dann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263aa53-ad43-416a-882b-0e43b2252e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80429e2b-e121-4e9e-ae96-45196b3e0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move below to stage4_regression or dataset_util    \n",
    "def store_data_meta_label(training_params):\n",
    "\n",
    "    data = data_loader('data', training_params['inputdir'])[:,None,:] # make middle dimension (channel) one\n",
    "    label_raw = data_loader('label', training_params['inputdir'])\n",
    "\n",
    "    # choose a small subset first\n",
    "    if training_params['tiny_dataset']:\n",
    "        data = data[:5000,:,:]\n",
    "        label_raw = label_raw[:5000,:]\n",
    "\n",
    "    # encode the labels so they are not stored in strings but in int\n",
    "    label_raw, training_params = encode_labels(label_raw, training_params)\n",
    "\n",
    "    # get actual label and actual meta\n",
    "\n",
    "    # select the relevant label (stored in output_names)\n",
    "\n",
    "    list_label = dataset_dict['list_label']\n",
    "\n",
    "    indices_label = []\n",
    "    for label_name in training_params['output_names']:\n",
    "        if 'reconstruction' in training_params['output_names']:\n",
    "            continue\n",
    "        i_label = list_label.index(label_name)\n",
    "        indices_label.append(i_label)\n",
    "\n",
    "    if training_params['output_names'][0]=='reconstruction':\n",
    "        label = data\n",
    "    else:\n",
    "        label = label_raw[:, indices_label]\n",
    "\n",
    "    # select the relevant meta (stored in output_names)\n",
    "    indices_meta = []\n",
    "    for meta_name in training_params['meta_names']:\n",
    "        i_meta = list_label.index(meta_name)\n",
    "        indices_meta.append(i_meta)\n",
    "\n",
    "    meta = label_raw[:, indices_meta]\n",
    "\n",
    "    # store them in training_params\n",
    "    training_params['data'] = data\n",
    "    training_params['label'] = label\n",
    "    training_params['meta'] = meta\n",
    "\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b03871-f6ae-467f-a9bb-fa978ef6560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "\n",
    "for training_params in [training_params_list[0]]:\n",
    "    # include device in training_params\n",
    "    # device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    training_params['device'] = device\n",
    "    \n",
    "    if 'training_mode' in training_params:\n",
    "        training_mode = training_params['training_mode']\n",
    "    else:\n",
    "        training_params = 'subject_ind'\n",
    "\n",
    "    training_params['CV_config'] = {\n",
    "        'CV': 1,\n",
    "    }\n",
    "\n",
    "    training_params['FS_RESAMPLE_DL'] = 100\n",
    "    \n",
    "    dataset_dict = data_loader('dataset_dict', inputdir).item()\n",
    "    training_params['dataset_dict'] = dataset_dict\n",
    "    \n",
    "    training_params['inputdir'] = inputdir\n",
    "    training_params['outputdir'] = outputdir\n",
    "\n",
    "    # load the data once only\n",
    "    training_params = store_data_meta_label(training_params)\n",
    "\n",
    "\n",
    "    dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n",
    "    # dataloaders, dataset_sizes, training_params = get_loaders(outputdir, training_params)\n",
    "    print('data dimensions are:', dataloaders['val'].dataset.data.shape)\n",
    "    print('feature dimensions are:', dataloaders['val'].dataset.feature.shape)\n",
    "    print('meta dimensions are:', dataloaders['val'].dataset.meta.shape)\n",
    "    print('label dimensions are:', dataloaders['val'].dataset.label.shape)\n",
    "\n",
    "    data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "    training_params['data_dimensions'] = list(data_dimensions) # should be (N_channel, N_samples)\n",
    "    del dataloaders\n",
    "\n",
    "#     sweep_name = training_params['sweep_name'] \n",
    "    \n",
    "    \n",
    "    training_params['featrue_extractor'] = extractor_dict[training_params['extractor_name']]\n",
    "    # if training_params['model_name'] == 'FeatureExtractor_CNN':\n",
    "    #     training_params['featrue_extractor'] = FeatureExtractor_CNN\n",
    "    # elif training_params['model_name'] == 'ResNet1D':\n",
    "    #     training_params['featrue_extractor'] = ResNet1D\n",
    "    # elif training_params['model_name'] == 'FeatureExtractor_CNN2':\n",
    "    #     training_params['featrue_extractor'] = FeatureExtractor_CNN2\n",
    "    # elif training_params['model_name'] == 'FeatureExtractor_CNNlight':\n",
    "    #     training_params['featrue_extractor'] = FeatureExtractor_CNNlight\n",
    "    \n",
    "\n",
    "\n",
    "    model_out_names = get_model_out_names(training_params)\n",
    "    training_params['model_out_names'] = model_out_names\n",
    "    \n",
    "\n",
    "\n",
    "    training_params['FS_Extracted'] = training_params['FS_RESAMPLE_DL'] / (training_params['stride']**training_params['n_block'])\n",
    "\n",
    "    \n",
    "    \n",
    "#     last_layer_dim = training_params['data_dimensions'][-1]\n",
    "#     for n in range(training_params['n_block']):\n",
    "#         last_layer_dim = round(last_layer_dim/training_params['stride'])\n",
    "\n",
    "#     training_params['last_layer_dim'] = last_layer_dim\n",
    "#     xf = np.linspace(0.0, 1.0/2.0*training_params['FS_Extracted'] , training_params['last_layer_dim']//2)*60    \n",
    "#     mask = (xf>=label_range_dict['HR_DL'][0]) & (xf<=label_range_dict['HR_DL'][1])\n",
    "\n",
    "#     training_params['xf'] = xf\n",
    "#     training_params['xf_masked'] = xf[mask]\n",
    "#     training_params['mask'] = mask\n",
    "\n",
    "# # training_params = training_params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1129a6-042a-4ddf-bce4-0ee36249a06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ebea72-c0a7-4eea-a200-ebf05e080415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a1d3c-806e-4c96-9cec-04a5082a4eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = nn.ModuleDict()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9166eaf8-600c-4172-8896-812dbf070948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa['decoder1'] = nn.Linear(50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb06cf5-2d91-4f5d-b2ac-288245cd6f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d9487-4c3f-49e3-8d96-d36de6bb5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e53c56-eb7b-4c9b-87f6-6ac881666a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db8b49-7a75-4135-81eb-2cbc1a200a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict['list_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981159a-d6c3-45f1-925b-9a19774d8252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976690a4-9037-47a4-a74e-61fafac9935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['race_encoder'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c856c93-d0a2-4eb4-940e-af8548746cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_demographic_processed = pd.read_csv(inputdir+'df_demographic_processed.csv.gz')  \n",
    "# df_demographic_processed = pd.read_csv('../../data/stage1/waveform/'+'df_demographic_processed.csv.gz')  \n",
    "# df_demographic_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43465cf-03d3-45db-9b2c-574fc4c574f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d2a9740-a03d-4242-b8be-937529b17886",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_lstm(training_params):\n",
    "    print('test_model_lstm')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    # prepare model\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    model = model.to(device).float()\n",
    "\n",
    "    # prepare data\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "    data = dataloaders['val'].dataset.data[:5,:,:]\n",
    "    data = torch.from_numpy(data)\n",
    "\n",
    "    feature = dataloaders['val'].dataset.feature[:5,:]\n",
    "    feature = torch.from_numpy(feature)\n",
    "\n",
    "    label = dataloaders['val'].dataset.label[:5,:]\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    data = data.to(device=device, dtype=torch.float)\n",
    "    feature = feature.to(device=device, dtype=torch.float)\n",
    "    label = label.to(device=device, dtype=torch.float)\n",
    "\n",
    "    # model inference\n",
    "    out = model(data, feature)\n",
    "\n",
    "    # compute loss\n",
    "    criterion = MultiTaskLoss(training_params)\n",
    "    losses = criterion(out, label)\n",
    "\n",
    "    # check losses\n",
    "    print(losses)\n",
    "    del model\n",
    "\n",
    "    \n",
    "def test_model(training_params):\n",
    "    print('test_model')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    print(model)\n",
    "    del model\n",
    "    \n",
    "def test_model_dann(training_params):\n",
    "    print('test_model_dann')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = resp_DANN(training_params=training_params)\n",
    "    print(model)\n",
    "\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    del model\n",
    "\n",
    "def test_PPG_compressor(training_params):\n",
    "    print('test_PPG_compressor')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = PPG_compressor(training_params=training_params)\n",
    "    print(model)\n",
    "\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    del model\n",
    "\n",
    "\n",
    "\n",
    "debug_model = True\n",
    "if debug_model==True:\n",
    "    if 'LSTM' in training_params['model_name']:\n",
    "        test_model_lstm(training_params)\n",
    "    elif 'DANN' in training_params['model_name']:\n",
    "        test_model_dann(training_params)\n",
    "    elif 'PPG_VAEcompressor' in training_params['model_name']:\n",
    "        test_PPG_compressor(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75729cb-ded3-48b3-9a3a-e9ca9f74116c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a5f1d-5110-486c-abe3-f1412d7f0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aa1_func(**kwargs):\n",
    "#     print(kwargs)\n",
    "#     print(kwargs['mu'])\n",
    "#     print(kwargs['logvar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d296d10-0cdc-43e0-b162-42c7d7c70de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa1_func(mu=1,logvar=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a53167-0d1e-4d17-a522-3850af36acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Conv1d(16, 33, 3, stride=1)\n",
    "# input = torch.randn(20, 16, 50)\n",
    "# output = m(input)\n",
    "\n",
    "# print(input.size(), output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436e1be-b4dd-45b6-ba08-53eb8996fa65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb575a5-6551-40ea-8a13-8b9a35be9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Conv1d(16, 32, 2, stride=1)\n",
    "# input = torch.randn(20, 16, 3)\n",
    "# output = m(input)\n",
    "\n",
    "# print(input.size(), output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e269e57-304d-4a80-8057-9bb9f9dcf40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c6ff2-70a7-4ea5-ad42-16fce5d7cef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cac636e-903a-43e9-80ea-6a391fbb5576",
   "metadata": {},
   "source": [
    "## make sure data can pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93633cfc-91c4-4da4-af3e-758f7dae2e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_flow = True\n",
    "\n",
    "if check_data_flow:\n",
    "\n",
    "    # dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    # dataloaders, dataset_sizes, training_params = get_loaders(outputdir, training_params)\n",
    "    dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n",
    "\n",
    "    data_val = dataloaders['val'].dataset.data[:5,:,:]\n",
    "    # data = torch\n",
    "    data_val = torch.from_numpy(data_val)\n",
    "    data_val = data_val.to(device=device, dtype=torch.float)\n",
    "\n",
    "    feature_val = dataloaders['val'].dataset.feature[:5,:]\n",
    "    # data = torch\n",
    "    feature_val = torch.from_numpy(feature_val)\n",
    "    feature_val = feature_val.to(device=device, dtype=torch.float)\n",
    "\n",
    "    label_val = dataloaders['val'].dataset.label[:5,:]\n",
    "    # data = torch\n",
    "    label_val = torch.from_numpy(label_val)\n",
    "    label_val = label_val.to(device=device, dtype=torch.float)\n",
    "\n",
    "    # model = resp_DANN(training_params=training_params)\n",
    "    model = PPG_compressor(training_params=training_params)\n",
    "\n",
    "    model = model.to(device).float()\n",
    "    output, feature_out, mu, logvar = model(data_val, feature_val)\n",
    "        # return output, feature_out, mu, logvar\n",
    "\n",
    "    # should be torch.Size([5, 1, 800]) torch.Size([5, 0]) torch.Size([5, 5]) torch.Size([5, 2]) torch.Size([5, 50])\n",
    "    # print(data_train.size(), feature_train.size(), label_train.size(), output['Race String-PPG'].size(), feature_out['PPG'].size())\n",
    "    \n",
    "    # should be torch.Size([5, 1, 800]) torch.Size([5, 0]) torch.Size([5, 5]) torch.Size([5, 500]) torch.Size([5, 1, 100]), torch.Size([5, 25]), torch.Size([5, 25])\n",
    "    print(data_val.size(), feature_val.size(), label_val.size(), output['reconstruction-PPG'].shape, feature_out['PPG'].size(), mu.size(), logvar.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22256c37-e6ce-4411-849b-bfcd97755a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoders.PPG.encoder_layer_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2f827-f4a2-4361-8f48-f88749291d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.encoders.PPG.output_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814452c-93ab-49e3-b842-e8bd73ef8f09",
   "metadata": {},
   "source": [
    "# activate wandb session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0687a4-7938-4113-8b9f-ff614b96823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['wandb']:\n",
    "    wandb.login()\n",
    "    os.environ[\"WANDB_DIR\"] = os.path.abspath(outputdir)\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'PPG_compression'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f07c1e4-479c-40fa-8c51-24a041b4081a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# define outputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b9f35-17f0-4256-9db6-d8e2bc311d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sweep_folder(training_params):\n",
    "    n_block = training_params['n_block']\n",
    "    inputs_combined = '+'.join([ i_name.split('_')[0] for i_name in training_params['input_names']])\n",
    "    auxillary_weight = training_params['loss_weights']['auxillary_task']\n",
    "    # adversarial_weight = training_params['adversarial_weight']\n",
    "    channel_n = training_params['channel_n']\n",
    "\n",
    "    list_act = '+'.join( [str(int) for int in training_params['activity_names']] )\n",
    "\n",
    "    sweep_folder = '{}blocks-{}-weight{}-{}ch-act{}'.format(n_block, inputs_combined, auxillary_weight, channel_n, list_act)\n",
    "\n",
    "    return sweep_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa4a8e-c06c-4072-ac57-9eea80bcf405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791287bb-430c-4994-a0c8-2334ec2aa237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputdirs(training_params):\n",
    "\n",
    "    outputdir = training_params['outputdir']\n",
    "    sweep_folder = get_sweep_folder(training_params)\n",
    "    outputdir_sweep = outputdir+'{}/'.format(sweep_folder)\n",
    "\n",
    "    outputdir_numeric = outputdir_sweep + 'numeric_results/'\n",
    "    if outputdir_numeric is not None:\n",
    "        if not os.path.exists(outputdir_numeric):\n",
    "            os.makedirs(outputdir_numeric)\n",
    "\n",
    "    outputdir_modelout = outputdir_sweep + 'model_output/'\n",
    "    if outputdir_modelout is not None:\n",
    "        if not os.path.exists(outputdir_modelout):\n",
    "            os.makedirs(outputdir_modelout)\n",
    "\n",
    "    outputdir_activation = outputdir_sweep + 'activation_layers/'\n",
    "    if outputdir_activation is not None:\n",
    "        if not os.path.exists(outputdir_activation):\n",
    "            os.makedirs(outputdir_activation)\n",
    "\n",
    "    outputdir_feature = outputdir_sweep + 'feature_visualization/'\n",
    "    if outputdir_feature is not None:\n",
    "        if not os.path.exists(outputdir_feature):\n",
    "            os.makedirs(outputdir_feature)\n",
    "\n",
    "    training_params['outputdir_sweep'] = outputdir_sweep\n",
    "    training_params['outputdir_numeric'] = outputdir_numeric\n",
    "    training_params['outputdir_modelout'] = outputdir_modelout\n",
    "    training_params['outputdir_activation'] = outputdir_activation\n",
    "    training_params['outputdir_feature'] = outputdir_feature\n",
    "\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a6507-40c6-4a85-9354-a7e25d89d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputdir_numeric = outputdir + 'numeric_results/'\n",
    "# if outputdir_numeric is not None:\n",
    "#     if not os.path.exists(outputdir_numeric):\n",
    "#         os.makedirs(outputdir_numeric)\n",
    "    \n",
    "# outputdir_modelout = outputdir + 'model_output/'\n",
    "# if outputdir_modelout is not None:\n",
    "#     if not os.path.exists(outputdir_modelout):\n",
    "#         os.makedirs(outputdir_modelout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8a97f7-205f-4abd-8b2d-3acc0c645eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressor_names(training_params):\n",
    "    training_params['regressor_names'] = []\n",
    "    main_task_name = training_params['output_names'][0]\n",
    "    \n",
    "    for output_name in training_params['output_names']:\n",
    "        if output_name == main_task_name:\n",
    "            training_params['regressor_names'].append(output_name)\n",
    "        else:\n",
    "            for input_name in training_params['input_names']:\n",
    "                training_params['regressor_names'].append(output_name + '-' + input_name)\n",
    "                \n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b5f7c-ec63-4e37-9169-bf6166dbd284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['regressor_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a3848b-d0c5-4886-8cec-6b43ed9ef43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6c4494-0ef6-48ee-b270-6060f5964982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd8e49-4d53-4b5d-ac23-e4770a359f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab3fa7-c625-4179-ab71-64766b52b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_conf(df_outputlabel, training_params, weighted=True, fig_name=None, show_plot=False, outputdir=None, log_wandb=False):\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(6, 5), dpi=100, facecolor='white')\n",
    "    # task_name = task.split('_')[0]\n",
    "\n",
    "    label = df_outputlabel['label']\n",
    "    label_est =  df_outputlabel['label_est']\n",
    "    \n",
    "    cm = metrics.confusion_matrix(label, label_est)\n",
    "    \n",
    "    if weighted:\n",
    "        cm = cm/cm.sum(axis=1)\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=training_params['race_encoder'].classes_ )\n",
    "\n",
    "    disp = disp.plot(include_values=True, cmap='Blues', ax=ax, xticks_rotation='60')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if fig_name is None:\n",
    "        fig_name = 'cm'\n",
    "\n",
    "    if log_wandb:\n",
    "        wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "    if outputdir is not None:\n",
    "        if not os.path.exists(outputdir):\n",
    "            os.makedirs(outputdir)\n",
    "        fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "    if show_plot == False:\n",
    "        plt.close(fig)\n",
    "        pyplot.close(fig)\n",
    "        plt.close('all')\n",
    "    # ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    # return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e095626d-f5b9-4cd6-923d-bf305c8a7e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_window(model, training_params, mode='train', fig_name=None, show_plot=False, outputdir=None, log_wandb=False):\n",
    "\n",
    "    inputdir = training_params['inputdir']\n",
    "    device = training_params['device']\n",
    "    # dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n",
    "\n",
    "    dataloader = dataloaders[mode]\n",
    "\n",
    "    data = torch.from_numpy(dataloader.dataset.data)\n",
    "    feature = torch.from_numpy(dataloader.dataset.feature)\n",
    "    data = data.to(device).float()\n",
    "    feature = feature.to(device).float()\n",
    "\n",
    "    label = dataloader.dataset.label\n",
    "\n",
    "    #     print(data.size(), feature.size(), label.shape)\n",
    "    #     print(data, feature, label)\n",
    "\n",
    "    # meta = dataloader.dataset.meta\n",
    "\n",
    "    model.eval()\n",
    "    #     _ = model(data, feature)\n",
    "    model = model.to(device).float()\n",
    "    output, feature_out, mu, logvar = model(data, feature)\n",
    "\n",
    "    data = data.cpu().detach().numpy()\n",
    "    output = output['reconstruction-PPG'].cpu().detach().numpy()\n",
    "    feature_out = feature_out['PPG'].cpu().detach().numpy()\n",
    "\n",
    "    N_ch = feature_out.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(N_ch+2,1,figsize=(5,(N_ch+2)), dpi=60) #   figsize=(width, height)\n",
    "\n",
    "\n",
    "\n",
    "    i_sample = 0\n",
    "\n",
    "    t_arr = np.arange(data.shape[-1])/FS_RESAMPLE_DL\n",
    "    \n",
    "    t_arr_feature = np.arange(feature_out.shape[-1]) / ( FS_RESAMPLE_DL / (training_params['stride']**(training_params['n_block']) ) )\n",
    "\n",
    "    \n",
    "    axes[0].plot(t_arr, data[i_sample,0,:])\n",
    "    axes[0].set_ylabel('input')\n",
    "    axes[0].set_xlim(t_arr.min(), t_arr.max()) # remove the weird white space at the beg and end of the plot\n",
    "\n",
    "\n",
    "    for i_ch in range(N_ch):\n",
    "        axes[i_ch+1].plot(t_arr_feature, feature_out[i_sample,i_ch,:])\n",
    "        axes[i_ch+1].set_ylabel('ch: {}'.format(i_ch))\n",
    "        axes[i_ch+1].set_xlim(t_arr_feature.min(), t_arr_feature.max()) # remove the weird white space at the beg and end of the plot\n",
    "\n",
    "    axes[-1].plot(t_arr, output[i_sample,:])\n",
    "    axes[-1].set_ylabel('output')\n",
    "    axes[-1].set_xlim(t_arr.min(), t_arr.max()) # remove the weird white space at the beg and end of the plot\n",
    "\n",
    "    for ax in axes:\n",
    "        ax_no_top_right(ax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if fig_name is None:\n",
    "        fig_name = 'signals'\n",
    "\n",
    "    if log_wandb:\n",
    "        wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "    if outputdir is not None:\n",
    "        if not os.path.exists(outputdir):\n",
    "            os.makedirs(outputdir)\n",
    "        fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "    if show_plot == False:\n",
    "        plt.close(fig)\n",
    "        pyplot.close(fig)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d66820-ad35-4e86-8960-030e73def3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced1a602-ebd2-40e6-97bc-bce5d31e5ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79071758-fe2b-483e-a702-0cedce9c0950",
   "metadata": {},
   "source": [
    "# train_master is a function that train and eval a model using training_params (which stores one HP set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f6d9c-4324-40e1-8a3e-476f0b64b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_auxillary = False\n",
    "\n",
    "def train_master(training_params):\n",
    "    \n",
    "    # training_params = get_regressor_names(training_params)\n",
    "\n",
    "   # TODO: change all to training_params['xxx'] = get_xxx(training_params)\n",
    "    training_params = get_outputdirs(training_params) # could be tricky since it changes several keys\n",
    "    training_params = get_regressor_names(training_params) # may not need this in this task\n",
    "    training_params['model_out_names'] = get_model_out_names(training_params)\n",
    "    # training_params['modality_dict'] = get_modality_dict(training_params)\n",
    "    # training_params = update_freq_meta(training_params) # could be tricky since it changes several keys\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pprint.pprint(training_params)\n",
    "    \n",
    "    df_performance_train = {}\n",
    "    df_performance_val = {}\n",
    "\n",
    "    df_outputlabel_train = {}\n",
    "    df_outputlabel_val = {}\n",
    "\n",
    "    for task in training_params['model_out_names']:\n",
    "\n",
    "        df_performance_train[task] = pd.DataFrame()\n",
    "        df_performance_val[task] = pd.DataFrame()\n",
    "\n",
    "        df_outputlabel_train[task] = pd.DataFrame()\n",
    "        df_outputlabel_val[task] = pd.DataFrame()\n",
    "\n",
    "        \n",
    "    main_task = training_params['output_names'][0].split('-')[0]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    N_CV = training_params['split_name_encoder'].classes_.shape[0] - 1\n",
    "    for i_CV in range(N_CV):\n",
    "        \n",
    "        if 'CV_max' in training_params:\n",
    "            if i_CV >= training_params['CV_max']:\n",
    "                continue\n",
    "\n",
    "        training_params['CV_config']['CV'] = i_CV\n",
    "\n",
    "        device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "        print('using device', device)\n",
    "\n",
    "\n",
    "        print('using model ', training_params['model_name'])\n",
    "\n",
    "        model = PPG_compressor(training_params=training_params)\n",
    "        \n",
    "        model = model.to(device).float()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'], weight_decay=0.01)\n",
    "#         criterion = MultiTaskLoss(training_params)\n",
    "        # criterion = CompressionLoss(training_params)\n",
    "        criterion = VAELoss(training_params)\n",
    "        \n",
    "        \n",
    "        training_params['criterion'] = criterion\n",
    "        training_params['optimizer'] = optimizer\n",
    "        training_params['inputdir'] = inputdir\n",
    "\n",
    "#         print( training_params['regressor_names'])\n",
    "        CV_dict = train_model(model, training_params, trainer, evaler, preder)\n",
    "\n",
    "#         print(CV_dict['performance_dict_val']['out_dict'])\n",
    "#         sys.exit()\n",
    "\n",
    "        plot_losses(CV_dict, outputdir=training_params['outputdir_sweep'], show_plot=False)\n",
    "\n",
    "#         TODO: fix this code\n",
    "#         print(training_params['regressor_names'])\n",
    "        \n",
    "#         for task in CV_dict['performance_dict_train']['out_dict'].keys():\n",
    "#         for task in training_params['regressor_names']:\n",
    "        for task in training_params['model_out_names']:\n",
    "            if 'domain' in task:\n",
    "                continue\n",
    "        \n",
    "#             print(task, df_performance_train)\n",
    "\n",
    "#         for task in training_params['tasks']:\n",
    "\n",
    "#             print( CV_dict['performance_dict_val']['out_dict'].keys(), CV_dict['performance_dict_val']['label_dict'].keys(), task)\n",
    "            label_est_val = CV_dict['performance_dict_val']['out_dict'][task]\n",
    "            label_val = CV_dict['performance_dict_val']['label_dict'][task]\n",
    "\n",
    "            label_est_train = CV_dict['performance_dict_train']['out_dict'][task]\n",
    "            label_train = CV_dict['performance_dict_train']['label_dict'][task]\n",
    "            \n",
    "            \n",
    "#             if 'domain' in task:\n",
    "#                 np.argmax(a, axis=1)\n",
    "            \n",
    "            \n",
    "#             # rescale the label after making estimations\n",
    "#             if 'perc' in training_params['output_names'][0]:\n",
    "#                 i_meta = training_params['meta_names'].index('EEavg_est')\n",
    "# #                 print(CV_dict['performance_dict_train']['meta_arr'], CV_dict['performance_dict_train']['meta_arr'].shape)\n",
    "#                 meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "#                 meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "#                 label_train = label_train*meta_train\n",
    "#                 label_val = label_val*meta_val\n",
    "#                 label_est_train = label_est_train*meta_train\n",
    "#                 label_est_val = label_est_val*meta_val\n",
    "#             elif 'weighted' in training_params['output_names'][0]:\n",
    "#                 i_meta = training_params['meta_names'].index('weight')\n",
    "#                 meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "#                 meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "#                 label_train = label_train*meta_train\n",
    "#                 label_val = label_val*meta_val\n",
    "#                 label_est_train = label_est_train*meta_train\n",
    "#                 label_est_val = label_est_val*meta_val\n",
    "\n",
    "                \n",
    "#             print(label_val, label_est_val)\n",
    "#             sys.exit()\n",
    "            \n",
    "            # get performance df for training and testing dataset\n",
    "#             df_performance_train[task] = df_performance_train[task].append( get_df_performance(label_train, label_est_train, i_CV, task), ignore_index=True )\n",
    "\n",
    "#             df_performance_train[task].to_csv(training_params['outputdir_numeric']  + 'df_performance_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "#             df_outputlabel_train[task] = df_outputlabel_train[task].append(\n",
    "#                 pd.DataFrame( {\n",
    "#                 'label_est': label_est_train,\n",
    "#                 'label': label_train,\n",
    "#                 'CV': [i_CV]*label_train.shape[0],\n",
    "#                 'task': [task]*label_train.shape[0]\n",
    "#                 }), ignore_index=True )\n",
    "\n",
    "#             df_outputlabel_train[task].to_csv(training_params['outputdir_numeric']  + 'df_outputlabel_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "            # df_performance_val[task] = df_performance_val[task].append( get_df_performance(label_val, label_est_val, i_CV, task), ignore_index=True )\n",
    "            # df_performance_val[task].to_csv(training_params['outputdir_numeric']  + 'df_performance_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "#             df_outputlabel_val[task] = df_outputlabel_val[task].append(\n",
    "#                 pd.DataFrame( {\n",
    "#                 'label_est': label_est_val,\n",
    "#                 'label': label_val,\n",
    "#                 'CV': [i_CV]*label_val.shape[0],\n",
    "#                 'task': [task]*label_val.shape[0]\n",
    "#                 }), ignore_index=True )\n",
    "\n",
    "#             df_outputlabel_val[task].to_csv(training_params['outputdir_numeric']  + 'df_outputlabel_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "            # plot performance training and testing dataset\n",
    "            if (main_task not in task) and (debug_auxillary==False):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            plot_one_window(model, training_params, mode='train', fig_name='signals_CV{}'.format(i_CV), outputdir=training_params['outputdir_modelout']+'train/', show_plot=False)\n",
    "\n",
    "            plot_one_window(model, training_params, mode='val', fig_name='signals_CV{}'.format(i_CV), outputdir=training_params['outputdir_modelout']+'val/', show_plot=False)\n",
    "\n",
    "            # sys.exit()\n",
    "\n",
    "            # plot_conf(df_outputlabel_train[task], training_params,  fig_name='cm_train', show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "            # plot_conf(df_outputlabel_val[task], training_params,  fig_name='cm_val', show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "            # sys.exit()\n",
    "\n",
    "            # plot_regression(df_outputlabel_train[task], df_performance_train[task], task, fig_name='regression_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "#             plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "            # plot_regression(df_outputlabel_val[task], df_performance_val[task], task, fig_name='regression_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "#             plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "#             plot_output(df_outputlabel_train[task], task, fig_name = 'outputINtime_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout)\n",
    "        \n",
    "        # check_featuremap(model, training_params, mode='worst', fig_name = 'DL_activation_{}_'.format(i_CV), outputdir=outputdir+'activation_layers_worst/{}/'.format(i_CV), show_plot=False)\n",
    "        # check_featuremap(model, training_params, mode='best', fig_name = 'DL_activation_{}_'.format(i_CV), outputdir=outputdir+'activation_layers_best/{}/'.format(i_CV), show_plot=False)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # for task in training_params['model_out_names']:\n",
    "    #     if main_task not in task:\n",
    "    #         continue\n",
    "#         if task!=main_task:\n",
    "#             continue\n",
    "#         plot_regression_all_agg(df_outputlabel_train[task], df_performance_train[task], fig_name='LinearR_agg_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "#         plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "\n",
    "#         plot_regression_all_agg(df_outputlabel_val[task], df_performance_val[task], fig_name='LinearR_agg_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "#         plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "\n",
    "        # plot_output(df_outputlabel_val[task], task, fig_name = 'outputINtime_val_{}'.format(task),  show_plot=False, outputdir=outputdir_modelout)\n",
    "\n",
    "#     plot_BA(df_outputlabel_val[main_task], main_task, fig_name='BA_val_{}'.format(main_task), show_plot=False, outputdir=outputdir+'model_output/', log_wandb=training_params['wandb'])\n",
    "#     plot_regression_all_agg(df_outputlabel_val[main_task], df_performance_val[main_task], outputdir=outputdir+'model_output/', show_plot=False, log_wandb=training_params['wandb'])\n",
    "\n",
    "    # log metrices on wnadb\n",
    "#     if training_params['wandb']==True:\n",
    "#         # W&B\n",
    "#         main_task = 'HR_patch-ECG_filt'\n",
    "        \n",
    "#         label = df_outputlabel_val[main_task]['label'].values\n",
    "#         label_est = df_outputlabel_val[main_task]['label_est'].values\n",
    "# #         print(label.shape, label)\n",
    "# #         print(label_est.shape, label_est)\n",
    "    \n",
    "#         PCC = get_PCC(label, label_est)\n",
    "#         Rsquared = get_CoeffDeterm(label, label_est)\n",
    "#         MAE, _ = get_MAE(label, label_est)\n",
    "#         RMSE = get_RMSE(label, label_est)\n",
    "#         MAPE, _ = get_MAPE(label, label_est)\n",
    "\n",
    "#         wandb.log(\n",
    "#             {\n",
    "#                 'val_MAE': MAE,\n",
    "#                 'val_RMSE': RMSE,\n",
    "#                 'val_MAPE': MAPE,\n",
    "#                 'val_PCC': PCC,\n",
    "#                 'val_Rsquared': Rsquared,\n",
    "#             })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220f303-f67e-4716-a877-078d930c48e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca38c404-cc3c-4804-b28d-555e9321564c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bb614b-b72a-4176-8bbb-7777ca1c5bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a3e57e-0bcf-4a8d-b568-af040f7e0999",
   "metadata": {},
   "source": [
    "# train_sweep is a function that wandb calls when changing to a new HP set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d26e6-2e0a-47c4-ac74-2eff268cb129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep(config=None):\n",
    "\n",
    "    with wandb.init(config=config, reinit=True, dir=outputdir):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        print(config)\n",
    "        \n",
    "        # init the model\n",
    "        for key in config.keys():\n",
    "            if key=='loss_weights':\n",
    "                training_params[key]['auxillary_task'] = config[key]\n",
    "            else:\n",
    "                training_params[key] = config[key]\n",
    "\n",
    "        train_master(training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6818fb-8fc2-4e74-a8e1-11f73e6b8bc3",
   "metadata": {},
   "source": [
    "# master wandb that select the HP set and ask the train_sweep to train and eval a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e000d-5714-4ab6-b4ae-a3b073681330",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_start = datetime.now(tz_NY)\n",
    "print(\"start time:\", datetime_start.strftime(\"%Y-%b-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a563124-e144-4c83-ad2e-803d5f5f4f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab3615a-1c0d-44d4-9ea1-7601a4627bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72aaae5-02d6-480c-bc78-f88ae53a8d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['wandb']:\n",
    "    print('sweeping for:', sweep_name)\n",
    "    # get the config of current sweep\n",
    "    sweep_config = training_params['sweep_config']    \n",
    "    # get the config of current sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, entity='inanlab', project='[PPG_compress] stage3_'+training_params['sweep_name'])\n",
    "    wandb.agent(sweep_id, train_sweep)\n",
    "else:\n",
    "    train_master(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b5071-cee5-4451-9c26-088cba509a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e251d-5296-4f6a-b84c-ab911143443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# num_of_gpus = torch.cuda.device_count()\n",
    "# print(num_of_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508f71d7-7bce-47d3-942a-858f083316da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e1f7a-4416-4660-9d7e-afb0ff1a51f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4299ee-505f-49cc-b01f-5f62f7577405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datetime_end = datetime.now(tz_NY)\n",
    "print(\"end time:\", datetime_end.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
    "\n",
    "duration = datetime_end-datetime_start\n",
    "duration_in_s = duration.total_seconds()\n",
    "days    = divmod(duration_in_s, 86400)        # Get days (without [0]!)\n",
    "hours   = divmod(days[1], 3600)               # Use remainder of days to calc hours\n",
    "minutes = divmod(hours[1], 60)                # Use remainder of hours to calc minutes\n",
    "seconds = divmod(minutes[1], 1)               # Use remainder of minutes to calc seconds\n",
    "print(\"Time between dates: %d days, %d hours, %d minutes and %d seconds\" % (days[0], hours[0], minutes[0], seconds[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941781c4-4efb-4476-b8e8-d83711ee023e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b0a40-4871-4c77-a2a1-b808284c24a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7064312-e4d9-4641-b74a-cb163e9a3b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74bdea1e-2ecb-4361-9dde-e08a9cfda690",
   "metadata": {},
   "source": [
    "# wrap it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e367f09-7678-436d-b074-bd3321b679ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_params['wandb']:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d60aa7-2cc0-4e90-b2cb-7c384defcd47",
   "metadata": {},
   "source": [
    "# time it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812002b-3454-4d30-bf7c-0adc4505f09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35c8af-b241-4704-bacd-b5a3b2d413d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6254cfe-2be2-4c2a-9049-3819f74e08e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696206e-8e2d-4fe3-8993-e1be484719d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc221e44-7049-478c-8be2-055f1737de41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab9853-46ff-47b2-b25b-b3ad96ce532d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774904ff-5e7c-446e-9105-6c690dd3c6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7573de8d-6d1b-42ed-af01-9be1e442cac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64949a1c-be4f-48d6-9d35-5d67b03633e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b16da8-20f9-4ca4-8c4c-1ee525cc2bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12adf142-431c-45cd-bb71-c08e98085fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e8e028-643d-4647-991a-0cb4f19c0206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4587914-5cd6-4bfb-a6d1-e5285620d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7771e-fece-489a-923e-c087603c85d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f6e4d-b63a-457c-a4e7-129d434d8aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d51587a-ab17-41f7-8774-2e613d347299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2f8fc-88a5-42ab-a6ea-3a3b59c6c340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98423264-2bb1-46ce-88f8-ab943055e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e79f596-092b-4bcc-9362-0cede3a8b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6acc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3b382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b9f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
