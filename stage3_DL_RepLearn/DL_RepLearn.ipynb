{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data\n",
    "# design model\n",
    "# test model\n",
    "# TODO: continue to debug model block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import json\n",
    "import pytz\n",
    "import pprint\n",
    "\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "matplotlib.rc( 'savefig', facecolor = 'white' )\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "# %pip install torch-summary\n",
    "from torchsummary import summary\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../') # add this line so data are visible in this file\n",
    "sys.path.append('../../') # add this line so data are visible in this file\n",
    "sys.path.append('../PhysioMC/') # add this line so data are visible in this file\n",
    "\n",
    "# from PatchWand import *\n",
    "from filters import *\n",
    "from setting import *\n",
    "# from preprocessing import *\n",
    "from ECG_module import *\n",
    "from dataIO import *\n",
    "from evaluate import *\n",
    "from stage1_PPG_analysis import *\n",
    "from plotting_tools import *\n",
    "from stage4_regression import *\n",
    "\n",
    "\n",
    "from DR_extension.training_util import *\n",
    "from DR_extension.dataset_util import *\n",
    "from DR_extension.evaluation_util import *\n",
    "from DR_extension.models import *\n",
    "from DR_extension.models_CNNlight import *\n",
    "\n",
    "from importlib import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # m = nn.Softmax(dim=-1)\n",
    "# input1 = torch.randn(64, 3, 10, 100)\n",
    "# input2 = torch.randn(64, 3, 10, 100)\n",
    "\n",
    "# attn = torch.matmul(input1, input2.transpose(2, 3))\n",
    "# # fc = nn.Linear(25, 37, bias=False)\n",
    "\n",
    "# print(attn.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fc(input).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.sum(axis=-1), output.sum(axis=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input_folder='../../data/stage3_DL_prepare/', output_folder='../../data/stage3_DL_RepLearn/', training_params_file='training_params_baseline.json')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='feature_learning')\n",
    "parser.add_argument('--input_folder', metavar='input_folder', help='input_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--output_folder', metavar='output_folder', help='output_folder',\n",
    "                    default='../')\n",
    "parser.add_argument('--training_params_file', metavar='training_params_file', help='training_params_file',\n",
    "                    default='training_params_list.json')\n",
    "\n",
    "\n",
    "# checklist 3: comment first line, uncomment second line\n",
    "# args = parser.parse_args(['--input_folder', '../../data/stage1/waveform/', \n",
    "args = parser.parse_args(['--input_folder', '../../data/stage3_DL_prepare/', \n",
    "                          '--output_folder', '../../data/stage3_DL_RepLearn/',\n",
    "                          '--training_params_file', 'training_params_baseline.json',\n",
    "                          # '--training_params_file', 'training_params_dummy.json',\n",
    "                         ])\n",
    "# args = parser.parse_args()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_id = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mchan/disparities_O2/repo/stage3_DL_RepLearn'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputdir = '../../data/stage1/waveform/'\n",
    "inputdir = args.input_folder\n",
    "outputdir = args.output_folder\n",
    "training_params_file = args.training_params_file\n",
    "\n",
    "# outputdir = '../../data/stage3_DL_RepLearn/'\n",
    "if not os.path.exists(outputdir):\n",
    "    os.makedirs(outputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_out_names(training_params):\n",
    "    model_out_names = []\n",
    "\n",
    "#     for output_name in training_params['output_names']:\n",
    "    for output_name in training_params['output_names']:\n",
    "        for input_name in training_params['input_names']:\n",
    "            model_out_names.append(output_name+'-{}'.format(input_name))\n",
    "    return model_out_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(label, training_params):\n",
    "\n",
    "    dataset_dict = training_params['dataset_dict']\n",
    "\n",
    "    race_encoder = LabelEncoder()\n",
    "    i_race = dataset_dict['list_label'].index('Race String')\n",
    "    label[:, i_race] = race_encoder.fit_transform(label[:, i_race])\n",
    "\n",
    "    PAT_ID_encoder = LabelEncoder()\n",
    "    i_PAT_ID = dataset_dict['list_label'].index('PAT_ID')\n",
    "    label[:, i_PAT_ID] = PAT_ID_encoder.fit_transform(label[:, i_PAT_ID])\n",
    "\n",
    "    split_name_encoder = LabelEncoder()\n",
    "    i_split_name = dataset_dict['list_label'].index('split_name')\n",
    "    label[:, i_split_name] = split_name_encoder.fit_transform(label[:, i_split_name])\n",
    "    \n",
    "    training_params['PAT_ID_encoder'] = PAT_ID_encoder\n",
    "    training_params['race_encoder'] = race_encoder\n",
    "    training_params['split_name_encoder'] = split_name_encoder\n",
    "\n",
    "    return label, training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = train_dann\n",
    "evaler = eval_dann\n",
    "preder = pred_dann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move below to stage4_regression or dataset_util    \n",
    "def store_data_meta_label(training_params):\n",
    "\n",
    "    data = data_loader('data', training_params['inputdir'])[:,None,:] # make middle dimension (channel) one\n",
    "    label_raw = data_loader('label', training_params['inputdir'])\n",
    "\n",
    "    # choose a small subset first\n",
    "    if training_params['tiny_dataset']:\n",
    "        data = data[:5000,:,:]\n",
    "        label_raw = label_raw[:5000,:]\n",
    "\n",
    "    # encode the labels so they are not stored in strings but in int\n",
    "    label_raw, training_params = encode_labels(label_raw, training_params)\n",
    "\n",
    "    # get actual label and actual meta\n",
    "\n",
    "    # select the relevant label (stored in output_names)\n",
    "\n",
    "    list_label = dataset_dict['list_label']\n",
    "\n",
    "    indices_label = []\n",
    "    for label_name in training_params['output_names']:\n",
    "        if 'reconstruction' in training_params['output_names']:\n",
    "            continue\n",
    "        i_label = list_label.index(label_name)\n",
    "        indices_label.append(i_label)\n",
    "\n",
    "    if training_params['output_names'][0]=='reconstruction':\n",
    "        label = data\n",
    "    else:\n",
    "        label = label_raw[:, indices_label]\n",
    "\n",
    "    # select the relevant meta (stored in output_names)\n",
    "    indices_meta = []\n",
    "    for meta_name in training_params['meta_names']:\n",
    "        i_meta = list_label.index(meta_name)\n",
    "        indices_meta.append(i_meta)\n",
    "\n",
    "    meta = label_raw[:, indices_meta]\n",
    "\n",
    "    # store them in training_params\n",
    "    training_params['data'] = data\n",
    "    training_params['label'] = label\n",
    "    training_params['meta'] = meta\n",
    "\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data dimensions are: (291, 1, 800)\n",
      "feature dimensions are: (291, 0)\n",
      "meta dimensions are: (291, 2)\n",
      "label dimensions are: (291, 1, 800)\n"
     ]
    }
   ],
   "source": [
    "with open(training_params_file) as json_file:\n",
    "    training_params_list = json.load(json_file)\n",
    "\n",
    "for training_params in [training_params_list[0]]:\n",
    "    # include device in training_params\n",
    "    device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    training_params['device'] = device\n",
    "    \n",
    "    training_params['sweep_name'] = training_params_file.split('_')[-1].split('.')[0]\n",
    "    \n",
    "    if 'training_mode' in training_params:\n",
    "        training_mode = training_params['training_mode']\n",
    "    else:\n",
    "        training_params = 'subject_ind'\n",
    "\n",
    "    training_params['CV_config'] = {\n",
    "        'CV': 1,\n",
    "    }\n",
    "\n",
    "    training_params['FS_RESAMPLE_DL'] = 100\n",
    "    \n",
    "    dataset_dict = data_loader('dataset_dict', inputdir).item()\n",
    "    training_params['dataset_dict'] = dataset_dict\n",
    "    \n",
    "    training_params['inputdir'] = inputdir\n",
    "    training_params['outputdir'] = outputdir\n",
    "\n",
    "    # load the data once only\n",
    "    training_params = store_data_meta_label(training_params)\n",
    "\n",
    "\n",
    "    dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n",
    "    # dataloaders, dataset_sizes, training_params = get_loaders(outputdir, training_params)\n",
    "    print('data dimensions are:', dataloaders['val'].dataset.data.shape)\n",
    "    print('feature dimensions are:', dataloaders['val'].dataset.feature.shape)\n",
    "    print('meta dimensions are:', dataloaders['val'].dataset.meta.shape)\n",
    "    print('label dimensions are:', dataloaders['val'].dataset.label.shape)\n",
    "\n",
    "    data_dimensions = dataloaders['train'].dataset.__getitem__(0)[0].size()\n",
    "    training_params['data_dimensions'] = list(data_dimensions) # should be (N_channel, N_samples)\n",
    "    del dataloaders\n",
    "\n",
    "#     sweep_name = training_params['sweep_name'] \n",
    "    \n",
    "    \n",
    "    training_params['featrue_extractor'] = extractor_dict[training_params['extractor_name']]\n",
    "    # if training_params['model_name'] == 'FeatureExtractor_CNN':\n",
    "    #     training_params['featrue_extractor'] = FeatureExtractor_CNN\n",
    "    # elif training_params['model_name'] == 'ResNet1D':\n",
    "    #     training_params['featrue_extractor'] = ResNet1D\n",
    "    # elif training_params['model_name'] == 'FeatureExtractor_CNN2':\n",
    "    #     training_params['featrue_extractor'] = FeatureExtractor_CNN2\n",
    "    # elif training_params['model_name'] == 'FeatureExtractor_CNNlight':\n",
    "    #     training_params['featrue_extractor'] = FeatureExtractor_CNNlight\n",
    "    \n",
    "\n",
    "\n",
    "    model_out_names = get_model_out_names(training_params)\n",
    "    training_params['model_out_names'] = model_out_names\n",
    "    \n",
    "\n",
    "\n",
    "    training_params['FS_Extracted'] = training_params['FS_RESAMPLE_DL'] / (training_params['stride']**training_params['n_block'])\n",
    "\n",
    "    \n",
    "    \n",
    "#     last_layer_dim = training_params['data_dimensions'][-1]\n",
    "#     for n in range(training_params['n_block']):\n",
    "#         last_layer_dim = round(last_layer_dim/training_params['stride'])\n",
    "\n",
    "#     training_params['last_layer_dim'] = last_layer_dim\n",
    "#     xf = np.linspace(0.0, 1.0/2.0*training_params['FS_Extracted'] , training_params['last_layer_dim']//2)*60    \n",
    "#     mask = (xf>=label_range_dict['HR_DL'][0]) & (xf<=label_range_dict['HR_DL'][1])\n",
    "\n",
    "#     training_params['xf'] = xf\n",
    "#     training_params['xf_masked'] = xf[mask]\n",
    "#     training_params['mask'] = mask\n",
    "\n",
    "# # training_params = training_params_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa['decoder1'] = nn.Linear(50,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_dict['list_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['race_encoder'].classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_demographic_processed = pd.read_csv(inputdir+'df_demographic_processed.csv.gz')  \n",
    "# df_demographic_processed = pd.read_csv('../../data/stage1/waveform/'+'df_demographic_processed.csv.gz')  \n",
    "# df_demographic_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_PPG_compressor\n",
      "using model  PPG_VAEcompressor\n",
      "feature_out_dim : 1792\n",
      "PPG_compressor(\n",
      "  (encoders): ModuleDict(\n",
      "    (PPG): Encoder(\n",
      "      (basicblock_list): ModuleList(\n",
      "        (0): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(1, 4, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(1, 4, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(1, 4, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(12, 4, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(4, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(4, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(4, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(8, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(8, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(8, 16, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(48, 16, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(16, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(16, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(16, 32, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(96, 32, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(32, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(32, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(32, 64, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(192, 64, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (5): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(64, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(64, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(64, 128, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(384, 128, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (6): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(5,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(9,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(128, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): MyAvgPool1dPadSame(\n",
      "              (avg_pool): AvgPool1d(kernel_size=(13,), stride=(2,), padding=(0,))\n",
      "            )\n",
      "            (1): Conv1d(128, 256, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(768, 256, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "      (condenser): Conv1d(256, 10, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      "  (Flatten): Flatten()\n",
      "  (fc1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (UnFlatten): UnFlatten()\n",
      "  (decoders): ModuleDict(\n",
      "    (reconstruction): Decoder(\n",
      "      (decondenser): ConvTranspose1d(10, 256, kernel_size=(7,), stride=(1,))\n",
      "      (basicblock_list): ModuleList(\n",
      "        (0): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Upsample(size=13, mode=linear)\n",
      "            (1): Conv1d(256, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Upsample(size=13, mode=linear)\n",
      "            (1): Conv1d(256, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Upsample(size=13, mode=linear)\n",
      "            (1): Conv1d(256, 128, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(384, 128, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (1): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Upsample(size=25, mode=linear)\n",
      "            (1): Conv1d(128, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Upsample(size=25, mode=linear)\n",
      "            (1): Conv1d(128, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Upsample(size=25, mode=linear)\n",
      "            (1): Conv1d(128, 64, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(192, 64, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (2): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Upsample(size=50, mode=linear)\n",
      "            (1): Conv1d(64, 32, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Upsample(size=50, mode=linear)\n",
      "            (1): Conv1d(64, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Upsample(size=50, mode=linear)\n",
      "            (1): Conv1d(64, 32, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(96, 32, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (3): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Upsample(size=100, mode=linear)\n",
      "            (1): Conv1d(32, 16, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Upsample(size=100, mode=linear)\n",
      "            (1): Conv1d(32, 16, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Upsample(size=100, mode=linear)\n",
      "            (1): Conv1d(32, 16, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(48, 16, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (4): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Upsample(size=200, mode=linear)\n",
      "            (1): Conv1d(16, 8, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Upsample(size=200, mode=linear)\n",
      "            (1): Conv1d(16, 8, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Upsample(size=200, mode=linear)\n",
      "            (1): Conv1d(16, 8, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(24, 8, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (5): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Upsample(size=400, mode=linear)\n",
      "            (1): Conv1d(8, 4, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Upsample(size=400, mode=linear)\n",
      "            (1): Conv1d(8, 4, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Upsample(size=400, mode=linear)\n",
      "            (1): Conv1d(8, 4, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(12, 4, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "        (6): InceptionBlock(\n",
      "          (conv_k1x1): Sequential(\n",
      "            (0): Upsample(size=800, mode=linear)\n",
      "            (1): Conv1d(4, 1, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "            (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k2x1): Sequential(\n",
      "            (0): Upsample(size=800, mode=linear)\n",
      "            (1): Conv1d(4, 1, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "            (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (conv_k3x1): Sequential(\n",
      "            (0): Upsample(size=800, mode=linear)\n",
      "            (1): Conv1d(4, 1, kernel_size=(13,), stride=(1,), padding=(6,))\n",
      "            (2): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (3): ReLU()\n",
      "          )\n",
      "          (ch_pooling): Conv1d(3, 1, kernel_size=(1,), stride=(1,))\n",
      "        )\n",
      "      )\n",
      "      (ch_pooling): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         AvgPool1d-1               [-1, 1, 400]               0\n",
      "MyAvgPool1dPadSame-2               [-1, 1, 400]               0\n",
      "            Conv1d-3               [-1, 4, 400]              24\n",
      "       BatchNorm1d-4               [-1, 4, 400]               8\n",
      "              ReLU-5               [-1, 4, 400]               0\n",
      "         AvgPool1d-6               [-1, 1, 400]               0\n",
      "MyAvgPool1dPadSame-7               [-1, 1, 400]               0\n",
      "            Conv1d-8               [-1, 4, 400]              40\n",
      "       BatchNorm1d-9               [-1, 4, 400]               8\n",
      "             ReLU-10               [-1, 4, 400]               0\n",
      "        AvgPool1d-11               [-1, 1, 400]               0\n",
      "MyAvgPool1dPadSame-12               [-1, 1, 400]               0\n",
      "           Conv1d-13               [-1, 4, 400]              56\n",
      "      BatchNorm1d-14               [-1, 4, 400]               8\n",
      "             ReLU-15               [-1, 4, 400]               0\n",
      "           Conv1d-16               [-1, 4, 400]              52\n",
      "   InceptionBlock-17               [-1, 4, 400]               0\n",
      "        AvgPool1d-18               [-1, 4, 200]               0\n",
      "MyAvgPool1dPadSame-19               [-1, 4, 200]               0\n",
      "           Conv1d-20               [-1, 8, 200]             168\n",
      "      BatchNorm1d-21               [-1, 8, 200]              16\n",
      "             ReLU-22               [-1, 8, 200]               0\n",
      "        AvgPool1d-23               [-1, 4, 200]               0\n",
      "MyAvgPool1dPadSame-24               [-1, 4, 200]               0\n",
      "           Conv1d-25               [-1, 8, 200]             296\n",
      "      BatchNorm1d-26               [-1, 8, 200]              16\n",
      "             ReLU-27               [-1, 8, 200]               0\n",
      "        AvgPool1d-28               [-1, 4, 200]               0\n",
      "MyAvgPool1dPadSame-29               [-1, 4, 200]               0\n",
      "           Conv1d-30               [-1, 8, 200]             424\n",
      "      BatchNorm1d-31               [-1, 8, 200]              16\n",
      "             ReLU-32               [-1, 8, 200]               0\n",
      "           Conv1d-33               [-1, 8, 200]             200\n",
      "   InceptionBlock-34               [-1, 8, 200]               0\n",
      "        AvgPool1d-35               [-1, 8, 100]               0\n",
      "MyAvgPool1dPadSame-36               [-1, 8, 100]               0\n",
      "           Conv1d-37              [-1, 16, 100]             656\n",
      "      BatchNorm1d-38              [-1, 16, 100]              32\n",
      "             ReLU-39              [-1, 16, 100]               0\n",
      "        AvgPool1d-40               [-1, 8, 100]               0\n",
      "MyAvgPool1dPadSame-41               [-1, 8, 100]               0\n",
      "           Conv1d-42              [-1, 16, 100]           1,168\n",
      "      BatchNorm1d-43              [-1, 16, 100]              32\n",
      "             ReLU-44              [-1, 16, 100]               0\n",
      "        AvgPool1d-45               [-1, 8, 100]               0\n",
      "MyAvgPool1dPadSame-46               [-1, 8, 100]               0\n",
      "           Conv1d-47              [-1, 16, 100]           1,680\n",
      "      BatchNorm1d-48              [-1, 16, 100]              32\n",
      "             ReLU-49              [-1, 16, 100]               0\n",
      "           Conv1d-50              [-1, 16, 100]             784\n",
      "   InceptionBlock-51              [-1, 16, 100]               0\n",
      "        AvgPool1d-52               [-1, 16, 50]               0\n",
      "MyAvgPool1dPadSame-53               [-1, 16, 50]               0\n",
      "           Conv1d-54               [-1, 32, 50]           2,592\n",
      "      BatchNorm1d-55               [-1, 32, 50]              64\n",
      "             ReLU-56               [-1, 32, 50]               0\n",
      "        AvgPool1d-57               [-1, 16, 50]               0\n",
      "MyAvgPool1dPadSame-58               [-1, 16, 50]               0\n",
      "           Conv1d-59               [-1, 32, 50]           4,640\n",
      "      BatchNorm1d-60               [-1, 32, 50]              64\n",
      "             ReLU-61               [-1, 32, 50]               0\n",
      "        AvgPool1d-62               [-1, 16, 50]               0\n",
      "MyAvgPool1dPadSame-63               [-1, 16, 50]               0\n",
      "           Conv1d-64               [-1, 32, 50]           6,688\n",
      "      BatchNorm1d-65               [-1, 32, 50]              64\n",
      "             ReLU-66               [-1, 32, 50]               0\n",
      "           Conv1d-67               [-1, 32, 50]           3,104\n",
      "   InceptionBlock-68               [-1, 32, 50]               0\n",
      "        AvgPool1d-69               [-1, 32, 25]               0\n",
      "MyAvgPool1dPadSame-70               [-1, 32, 25]               0\n",
      "           Conv1d-71               [-1, 64, 25]          10,304\n",
      "      BatchNorm1d-72               [-1, 64, 25]             128\n",
      "             ReLU-73               [-1, 64, 25]               0\n",
      "        AvgPool1d-74               [-1, 32, 25]               0\n",
      "MyAvgPool1dPadSame-75               [-1, 32, 25]               0\n",
      "           Conv1d-76               [-1, 64, 25]          18,496\n",
      "      BatchNorm1d-77               [-1, 64, 25]             128\n",
      "             ReLU-78               [-1, 64, 25]               0\n",
      "        AvgPool1d-79               [-1, 32, 25]               0\n",
      "MyAvgPool1dPadSame-80               [-1, 32, 25]               0\n",
      "           Conv1d-81               [-1, 64, 25]          26,688\n",
      "      BatchNorm1d-82               [-1, 64, 25]             128\n",
      "             ReLU-83               [-1, 64, 25]               0\n",
      "           Conv1d-84               [-1, 64, 25]          12,352\n",
      "   InceptionBlock-85               [-1, 64, 25]               0\n",
      "        AvgPool1d-86               [-1, 64, 13]               0\n",
      "MyAvgPool1dPadSame-87               [-1, 64, 13]               0\n",
      "           Conv1d-88              [-1, 128, 13]          41,088\n",
      "      BatchNorm1d-89              [-1, 128, 13]             256\n",
      "             ReLU-90              [-1, 128, 13]               0\n",
      "        AvgPool1d-91               [-1, 64, 13]               0\n",
      "MyAvgPool1dPadSame-92               [-1, 64, 13]               0\n",
      "           Conv1d-93              [-1, 128, 13]          73,856\n",
      "      BatchNorm1d-94              [-1, 128, 13]             256\n",
      "             ReLU-95              [-1, 128, 13]               0\n",
      "        AvgPool1d-96               [-1, 64, 13]               0\n",
      "MyAvgPool1dPadSame-97               [-1, 64, 13]               0\n",
      "           Conv1d-98              [-1, 128, 13]         106,624\n",
      "      BatchNorm1d-99              [-1, 128, 13]             256\n",
      "            ReLU-100              [-1, 128, 13]               0\n",
      "          Conv1d-101              [-1, 128, 13]          49,280\n",
      "  InceptionBlock-102              [-1, 128, 13]               0\n",
      "       AvgPool1d-103               [-1, 128, 7]               0\n",
      "MyAvgPool1dPadSame-104               [-1, 128, 7]               0\n",
      "          Conv1d-105               [-1, 256, 7]         164,096\n",
      "     BatchNorm1d-106               [-1, 256, 7]             512\n",
      "            ReLU-107               [-1, 256, 7]               0\n",
      "       AvgPool1d-108               [-1, 128, 7]               0\n",
      "MyAvgPool1dPadSame-109               [-1, 128, 7]               0\n",
      "          Conv1d-110               [-1, 256, 7]         295,168\n",
      "     BatchNorm1d-111               [-1, 256, 7]             512\n",
      "            ReLU-112               [-1, 256, 7]               0\n",
      "       AvgPool1d-113               [-1, 128, 7]               0\n",
      "MyAvgPool1dPadSame-114               [-1, 128, 7]               0\n",
      "          Conv1d-115               [-1, 256, 7]         426,240\n",
      "     BatchNorm1d-116               [-1, 256, 7]             512\n",
      "            ReLU-117               [-1, 256, 7]               0\n",
      "          Conv1d-118               [-1, 256, 7]         196,864\n",
      "  InceptionBlock-119               [-1, 256, 7]               0\n",
      "          Conv1d-120                [-1, 10, 1]          17,930\n",
      "         Encoder-121                [-1, 10, 1]               0\n",
      "         Flatten-122                   [-1, 10]               0\n",
      "          Linear-123                   [-1, 10]             110\n",
      "          Linear-124                   [-1, 10]             110\n",
      "          Linear-125                   [-1, 10]             110\n",
      "       UnFlatten-126                [-1, 10, 1]               0\n",
      " ConvTranspose1d-127               [-1, 256, 7]          18,176\n",
      "        Upsample-128              [-1, 256, 13]               0\n",
      "          Conv1d-129              [-1, 128, 13]         163,968\n",
      "     BatchNorm1d-130              [-1, 128, 13]             256\n",
      "            ReLU-131              [-1, 128, 13]               0\n",
      "        Upsample-132              [-1, 256, 13]               0\n",
      "          Conv1d-133              [-1, 128, 13]         295,040\n",
      "     BatchNorm1d-134              [-1, 128, 13]             256\n",
      "            ReLU-135              [-1, 128, 13]               0\n",
      "        Upsample-136              [-1, 256, 13]               0\n",
      "          Conv1d-137              [-1, 128, 13]         426,112\n",
      "     BatchNorm1d-138              [-1, 128, 13]             256\n",
      "            ReLU-139              [-1, 128, 13]               0\n",
      "          Conv1d-140              [-1, 128, 13]          49,280\n",
      "  InceptionBlock-141              [-1, 128, 13]               0\n",
      "        Upsample-142              [-1, 128, 25]               0\n",
      "          Conv1d-143               [-1, 64, 25]          41,024\n",
      "     BatchNorm1d-144               [-1, 64, 25]             128\n",
      "            ReLU-145               [-1, 64, 25]               0\n",
      "        Upsample-146              [-1, 128, 25]               0\n",
      "          Conv1d-147               [-1, 64, 25]          73,792\n",
      "     BatchNorm1d-148               [-1, 64, 25]             128\n",
      "            ReLU-149               [-1, 64, 25]               0\n",
      "        Upsample-150              [-1, 128, 25]               0\n",
      "          Conv1d-151               [-1, 64, 25]         106,560\n",
      "     BatchNorm1d-152               [-1, 64, 25]             128\n",
      "            ReLU-153               [-1, 64, 25]               0\n",
      "          Conv1d-154               [-1, 64, 25]          12,352\n",
      "  InceptionBlock-155               [-1, 64, 25]               0\n",
      "        Upsample-156               [-1, 64, 50]               0\n",
      "          Conv1d-157               [-1, 32, 50]          10,272\n",
      "     BatchNorm1d-158               [-1, 32, 50]              64\n",
      "            ReLU-159               [-1, 32, 50]               0\n",
      "        Upsample-160               [-1, 64, 50]               0\n",
      "          Conv1d-161               [-1, 32, 50]          18,464\n",
      "     BatchNorm1d-162               [-1, 32, 50]              64\n",
      "            ReLU-163               [-1, 32, 50]               0\n",
      "        Upsample-164               [-1, 64, 50]               0\n",
      "          Conv1d-165               [-1, 32, 50]          26,656\n",
      "     BatchNorm1d-166               [-1, 32, 50]              64\n",
      "            ReLU-167               [-1, 32, 50]               0\n",
      "          Conv1d-168               [-1, 32, 50]           3,104\n",
      "  InceptionBlock-169               [-1, 32, 50]               0\n",
      "        Upsample-170              [-1, 32, 100]               0\n",
      "          Conv1d-171              [-1, 16, 100]           2,576\n",
      "     BatchNorm1d-172              [-1, 16, 100]              32\n",
      "            ReLU-173              [-1, 16, 100]               0\n",
      "        Upsample-174              [-1, 32, 100]               0\n",
      "          Conv1d-175              [-1, 16, 100]           4,624\n",
      "     BatchNorm1d-176              [-1, 16, 100]              32\n",
      "            ReLU-177              [-1, 16, 100]               0\n",
      "        Upsample-178              [-1, 32, 100]               0\n",
      "          Conv1d-179              [-1, 16, 100]           6,672\n",
      "     BatchNorm1d-180              [-1, 16, 100]              32\n",
      "            ReLU-181              [-1, 16, 100]               0\n",
      "          Conv1d-182              [-1, 16, 100]             784\n",
      "  InceptionBlock-183              [-1, 16, 100]               0\n",
      "        Upsample-184              [-1, 16, 200]               0\n",
      "          Conv1d-185               [-1, 8, 200]             648\n",
      "     BatchNorm1d-186               [-1, 8, 200]              16\n",
      "            ReLU-187               [-1, 8, 200]               0\n",
      "        Upsample-188              [-1, 16, 200]               0\n",
      "          Conv1d-189               [-1, 8, 200]           1,160\n",
      "     BatchNorm1d-190               [-1, 8, 200]              16\n",
      "            ReLU-191               [-1, 8, 200]               0\n",
      "        Upsample-192              [-1, 16, 200]               0\n",
      "          Conv1d-193               [-1, 8, 200]           1,672\n",
      "     BatchNorm1d-194               [-1, 8, 200]              16\n",
      "            ReLU-195               [-1, 8, 200]               0\n",
      "          Conv1d-196               [-1, 8, 200]             200\n",
      "  InceptionBlock-197               [-1, 8, 200]               0\n",
      "        Upsample-198               [-1, 8, 400]               0\n",
      "          Conv1d-199               [-1, 4, 400]             164\n",
      "     BatchNorm1d-200               [-1, 4, 400]               8\n",
      "            ReLU-201               [-1, 4, 400]               0\n",
      "        Upsample-202               [-1, 8, 400]               0\n",
      "          Conv1d-203               [-1, 4, 400]             292\n",
      "     BatchNorm1d-204               [-1, 4, 400]               8\n",
      "            ReLU-205               [-1, 4, 400]               0\n",
      "        Upsample-206               [-1, 8, 400]               0\n",
      "          Conv1d-207               [-1, 4, 400]             420\n",
      "     BatchNorm1d-208               [-1, 4, 400]               8\n",
      "            ReLU-209               [-1, 4, 400]               0\n",
      "          Conv1d-210               [-1, 4, 400]              52\n",
      "  InceptionBlock-211               [-1, 4, 400]               0\n",
      "        Upsample-212               [-1, 4, 800]               0\n",
      "          Conv1d-213               [-1, 1, 800]              21\n",
      "     BatchNorm1d-214               [-1, 1, 800]               2\n",
      "            ReLU-215               [-1, 1, 800]               0\n",
      "        Upsample-216               [-1, 4, 800]               0\n",
      "          Conv1d-217               [-1, 1, 800]              37\n",
      "     BatchNorm1d-218               [-1, 1, 800]               2\n",
      "            ReLU-219               [-1, 1, 800]               0\n",
      "        Upsample-220               [-1, 4, 800]               0\n",
      "          Conv1d-221               [-1, 1, 800]              53\n",
      "     BatchNorm1d-222               [-1, 1, 800]               2\n",
      "            ReLU-223               [-1, 1, 800]               0\n",
      "          Conv1d-224               [-1, 1, 800]               4\n",
      "  InceptionBlock-225               [-1, 1, 800]               0\n",
      "          Conv1d-226               [-1, 1, 800]               2\n",
      "         Decoder-227                  [-1, 800]               0\n",
      "================================================================\n",
      "Total params: 2,730,635\n",
      "Trainable params: 2,730,635\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.63\n",
      "Params size (MB): 10.42\n",
      "Estimated Total Size (MB): 13.04\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def test_model_lstm(training_params):\n",
    "    print('test_model_lstm')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    # prepare model\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    model = model.to(device).float()\n",
    "\n",
    "    # prepare data\n",
    "    dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "\n",
    "    data = dataloaders['val'].dataset.data[:5,:,:]\n",
    "    data = torch.from_numpy(data)\n",
    "\n",
    "    feature = dataloaders['val'].dataset.feature[:5,:]\n",
    "    feature = torch.from_numpy(feature)\n",
    "\n",
    "    label = dataloaders['val'].dataset.label[:5,:]\n",
    "    label = torch.from_numpy(label)\n",
    "\n",
    "    data = data.to(device=device, dtype=torch.float)\n",
    "    feature = feature.to(device=device, dtype=torch.float)\n",
    "    label = label.to(device=device, dtype=torch.float)\n",
    "\n",
    "    # model inference\n",
    "    out = model(data, feature)\n",
    "\n",
    "    # compute loss\n",
    "    criterion = MultiTaskLoss(training_params)\n",
    "    losses = criterion(out, label)\n",
    "\n",
    "    # check losses\n",
    "    print(losses)\n",
    "    del model\n",
    "\n",
    "    \n",
    "def test_model(training_params):\n",
    "    print('test_model')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = resp_multiverse(training_params=training_params)\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    print(model)\n",
    "    del model\n",
    "    \n",
    "def test_model_dann(training_params):\n",
    "    print('test_model_dann')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = resp_DANN(training_params=training_params)\n",
    "    print(model)\n",
    "\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    del model\n",
    "\n",
    "def test_PPG_compressor(training_params):\n",
    "    print('test_PPG_compressor')\n",
    "    print('using model ', training_params['model_name'])\n",
    "\n",
    "    model = PPG_compressor(training_params=training_params)\n",
    "    print(model)\n",
    "\n",
    "    summary(model, input_size=[tuple(training_params['data_dimensions']), (model.N_features,1)], device='cpu')\n",
    "    del model\n",
    "\n",
    "\n",
    "\n",
    "debug_model = True\n",
    "if debug_model==True:\n",
    "    if 'LSTM' in training_params['model_name']:\n",
    "        test_model_lstm(training_params)\n",
    "    elif 'DANN' in training_params['model_name']:\n",
    "        test_model_dann(training_params)\n",
    "    elif 'PPG_VAEcompressor' in training_params['model_name']:\n",
    "        test_PPG_compressor(training_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure data can pass through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_out_dim : 1792\n",
      "torch.Size([5, 1, 800]) torch.Size([5, 0]) torch.Size([5, 1, 800]) torch.Size([5, 800]) torch.Size([5, 10, 1]) torch.Size([5, 10]) torch.Size([5, 10])\n",
      "encoder_layer_dims: [800, 400, 200, 100, 50, 25, 13, 7]\n",
      "output_channels: [4, 8, 16, 32, 64, 128, 256]\n"
     ]
    }
   ],
   "source": [
    "check_data_flow = True\n",
    "\n",
    "if check_data_flow:\n",
    "\n",
    "    # dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    # dataloaders, dataset_sizes, training_params = get_loaders(outputdir, training_params)\n",
    "    dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n",
    "\n",
    "    data_val = dataloaders['val'].dataset.data[:5,:,:]\n",
    "    data_val = torch.from_numpy(data_val)\n",
    "    data_val = data_val.to(device=device, dtype=torch.float)\n",
    "\n",
    "    feature_val = dataloaders['val'].dataset.feature[:5,:]\n",
    "    feature_val = torch.from_numpy(feature_val)\n",
    "    feature_val = feature_val.to(device=device, dtype=torch.float)\n",
    "\n",
    "    label_val = dataloaders['val'].dataset.label[:5,:]\n",
    "    label_val = torch.from_numpy(label_val)\n",
    "    label_val = label_val.to(device=device, dtype=torch.float)\n",
    "\n",
    "    # model = resp_DANN(training_params=training_params)\n",
    "    model = PPG_compressor(training_params=training_params)\n",
    "\n",
    "    model = model.to(device).float()\n",
    "    output, feature_out, mu, logvar = model(data_val, feature_val)\n",
    "\n",
    "    # should be torch.Size([5, 1, 800]) torch.Size([5, 0]) torch.Size([5, 5]) torch.Size([5, 2]) torch.Size([5, 50])\n",
    "    # print(data_train.size(), feature_train.size(), label_train.size(), output['Race String-PPG'].size(), feature_out['PPG'].size())\n",
    "    \n",
    "    # should be torch.Size([5, 1, 800]) torch.Size([5, 0]) torch.Size([5, 5]) torch.Size([5, 500]) torch.Size([5, 1, 100]), torch.Size([5, 25]), torch.Size([5, 25])\n",
    "    print(data_val.size(), feature_val.size(), label_val.size(), output['reconstruction-PPG'].shape, feature_out['PPG'].size(), mu.size(), logvar.size())\n",
    "    print('encoder_layer_dims:', model.encoders.PPG.encoder_layer_dims)\n",
    "    print('output_channels:', model.encoders.PPG.output_channels)\n",
    "    \n",
    "    del model\n",
    "    del dataloaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activate wandb session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_params['wandb']:\n",
    "    wandb.login()\n",
    "    os.environ[\"WANDB_DIR\"] = os.path.abspath(outputdir)\n",
    "    os.environ[\"WANDB_NOTEBOOK_NAME\"] = 'PPG_compression'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# define outputdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sweep_folder(training_params):\n",
    "    n_block = training_params['n_block']\n",
    "    inputs_combined = '+'.join([ i_name.split('_')[0] for i_name in training_params['input_names']])\n",
    "    auxillary_weight = training_params['loss_weights']['auxillary_task']\n",
    "    # adversarial_weight = training_params['adversarial_weight']\n",
    "    channel_n = training_params['channel_n']\n",
    "\n",
    "    list_act = '+'.join( [str(int) for int in training_params['activity_names']] )\n",
    "\n",
    "    sweep_folder = '{}blocks-{}-weight{}-{}ch-act{}'.format(n_block, inputs_combined, auxillary_weight, channel_n, list_act)\n",
    "\n",
    "    return sweep_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputdirs(training_params):\n",
    "\n",
    "    outputdir = training_params['outputdir']\n",
    "    sweep_folder = get_sweep_folder(training_params)\n",
    "    outputdir_sweep = outputdir+'{}/'.format(sweep_folder)\n",
    "\n",
    "    outputdir_numeric = outputdir_sweep + 'numeric_results/'\n",
    "    if outputdir_numeric is not None:\n",
    "        if not os.path.exists(outputdir_numeric):\n",
    "            os.makedirs(outputdir_numeric)\n",
    "\n",
    "        if not os.path.exists(outputdir_numeric+'train/'):\n",
    "            os.makedirs(outputdir_numeric+'train/')\n",
    "            \n",
    "        if not os.path.exists(outputdir_numeric+'val/'):\n",
    "            os.makedirs(outputdir_numeric+'val/')\n",
    "            \n",
    "            \n",
    "\n",
    "    outputdir_modelout = outputdir_sweep + 'model_output/'\n",
    "    if outputdir_modelout is not None:\n",
    "        if not os.path.exists(outputdir_modelout):\n",
    "            os.makedirs(outputdir_modelout)\n",
    "\n",
    "    outputdir_activation = outputdir_sweep + 'activation_layers/'\n",
    "    if outputdir_activation is not None:\n",
    "        if not os.path.exists(outputdir_activation):\n",
    "            os.makedirs(outputdir_activation)\n",
    "\n",
    "    outputdir_feature = outputdir_sweep + 'feature_visualization/'\n",
    "    if outputdir_feature is not None:\n",
    "        if not os.path.exists(outputdir_feature):\n",
    "            os.makedirs(outputdir_feature)\n",
    "\n",
    "    training_params['outputdir_sweep'] = outputdir_sweep\n",
    "    training_params['outputdir_numeric'] = outputdir_numeric\n",
    "    training_params['outputdir_modelout'] = outputdir_modelout\n",
    "    training_params['outputdir_activation'] = outputdir_activation\n",
    "    training_params['outputdir_feature'] = outputdir_feature\n",
    "\n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regressor_names(training_params):\n",
    "    training_params['regressor_names'] = []\n",
    "    main_task_name = training_params['output_names'][0]\n",
    "    \n",
    "    for output_name in training_params['output_names']:\n",
    "        if output_name == main_task_name:\n",
    "            training_params['regressor_names'].append(output_name)\n",
    "        else:\n",
    "            for input_name in training_params['input_names']:\n",
    "                training_params['regressor_names'].append(output_name + '-' + input_name)\n",
    "                \n",
    "    return training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params['regressor_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_conf(df_outputlabel, training_params, weighted=True, fig_name=None, show_plot=False, outputdir=None, log_wandb=False):\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(6, 5), dpi=100, facecolor='white')\n",
    "    # task_name = task.split('_')[0]\n",
    "\n",
    "    label = df_outputlabel['label']\n",
    "    label_est =  df_outputlabel['label_est']\n",
    "    \n",
    "    cm = metrics.confusion_matrix(label, label_est)\n",
    "    \n",
    "    if weighted:\n",
    "        cm = cm/cm.sum(axis=1)\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                  display_labels=training_params['race_encoder'].classes_ )\n",
    "\n",
    "    disp = disp.plot(include_values=True, cmap='Blues', ax=ax, xticks_rotation='60')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    if fig_name is None:\n",
    "        fig_name = 'cm'\n",
    "\n",
    "    if log_wandb:\n",
    "        wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "    if outputdir is not None:\n",
    "        if not os.path.exists(outputdir):\n",
    "            os.makedirs(outputdir)\n",
    "        fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "    if show_plot == False:\n",
    "        plt.close(fig)\n",
    "        pyplot.close(fig)\n",
    "        plt.close('all')\n",
    "    # ax.set_title(title, fontsize=15)\n",
    "    \n",
    "    # return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_window(model, training_params, mode='train', fig_name=None, show_plot=False, outputdir=None, log_wandb=False):\n",
    "\n",
    "    inputdir = training_params['inputdir']\n",
    "    device = training_params['device']\n",
    "    # dataloaders, dataset_sizes = get_loaders(inputdir, training_params)\n",
    "    dataloaders, dataset_sizes, training_params = get_loaders(training_params)\n",
    "\n",
    "    dataloader = dataloaders[mode]\n",
    "\n",
    "    data = torch.from_numpy(dataloader.dataset.data)\n",
    "    feature = torch.from_numpy(dataloader.dataset.feature)\n",
    "    data = data.to(device).float()\n",
    "    feature = feature.to(device).float()\n",
    "\n",
    "    label = dataloader.dataset.label\n",
    "\n",
    "    #     print(data.size(), feature.size(), label.shape)\n",
    "    #     print(data, feature, label)\n",
    "\n",
    "    # meta = dataloader.dataset.meta\n",
    "\n",
    "    model.eval()\n",
    "    #     _ = model(data, feature)\n",
    "    model = model.to(device).float()\n",
    "    output, feature_out, mu, logvar = model(data, feature)\n",
    "\n",
    "    data = data.cpu().detach().numpy()\n",
    "    output = output['reconstruction-PPG'].cpu().detach().numpy()\n",
    "    feature_out = feature_out['PPG'].cpu().detach().numpy()\n",
    "\n",
    "    N_ch = feature_out.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(N_ch+2,1,figsize=(5,(N_ch+2)), dpi=60) #   figsize=(width, height)\n",
    "\n",
    "\n",
    "\n",
    "    i_sample = 0\n",
    "\n",
    "    t_arr = np.arange(data.shape[-1])/FS_RESAMPLE_DL\n",
    "    \n",
    "#     print(feature_out.shape)\n",
    "#     sys.exit()\n",
    "    t_arr_feature = np.arange(feature_out.shape[-1]) / ( FS_RESAMPLE_DL / (training_params['stride']**(training_params['n_block']) ) )\n",
    "\n",
    "    \n",
    "    axes[0].plot(t_arr, data[i_sample,0,:])\n",
    "    axes[0].set_ylabel('input')\n",
    "    axes[0].set_xlim(t_arr.min(), t_arr.max()) # remove the weird white space at the beg and end of the plot\n",
    "\n",
    "\n",
    "#     for i_ch in range(N_ch):\n",
    "#         axes[i_ch+1].plot(t_arr_feature, feature_out[i_sample,i_ch,:])\n",
    "#         axes[i_ch+1].set_ylabel('ch: {}'.format(i_ch))\n",
    "        \n",
    "# #         print(t_arr_feature.min(), t_arr_feature.max() )\n",
    "# #         sys.exit()\n",
    "#         axes[i_ch+1].set_xlim(t_arr_feature.min(), t_arr_feature.max()) # remove the weird white space at the beg and end of the plot\n",
    "\n",
    "    axes[-1].plot(t_arr, output[i_sample,:])\n",
    "    axes[-1].set_ylabel('output')\n",
    "    axes[-1].set_xlim(t_arr.min(), t_arr.max()) # remove the weird white space at the beg and end of the plot\n",
    "\n",
    "    for ax in axes:\n",
    "        ax_no_top_right(ax)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    if fig_name is None:\n",
    "        fig_name = 'signals'\n",
    "\n",
    "    if log_wandb:\n",
    "        wandb.log({fig_name: wandb.Image(fig)})\n",
    "\n",
    "    if outputdir is not None:\n",
    "        if not os.path.exists(outputdir):\n",
    "            os.makedirs(outputdir)\n",
    "        fig.savefig(outputdir + fig_name + '.png', facecolor=fig.get_facecolor())\n",
    "\n",
    "    if show_plot == False:\n",
    "        plt.close(fig)\n",
    "        pyplot.close(fig)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hidden(mu, logvar):\n",
    "    # mu dim: (N_batch, N_dim)\n",
    "    # logvar dim: (N_batch, N_dim)\n",
    "    \n",
    "    fig, axes = plt.subplots(N_ch+2,1,figsize=(5,(N_ch+2)), dpi=60) #   figsize=(width, height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_master is a function that train and eval a model using training_params (which stores one HP set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_auxillary = False\n",
    "\n",
    "def train_master(training_params):\n",
    "\n",
    "   # TODO: change all to training_params['xxx'] = get_xxx(training_params)\n",
    "    training_params = get_outputdirs(training_params) # could be tricky since it changes several keys\n",
    "    training_params = get_regressor_names(training_params) # may not need this in this task\n",
    "    training_params['model_out_names'] = get_model_out_names(training_params)\n",
    "    # training_params['modality_dict'] = get_modality_dict(training_params)\n",
    "    \n",
    "    # pprint.pprint(training_params)\n",
    "    \n",
    "    df_performance_train = {}\n",
    "    df_performance_val = {}\n",
    "\n",
    "    df_outputlabel_train = {}\n",
    "    df_outputlabel_val = {}\n",
    "\n",
    "    for task in training_params['model_out_names']:\n",
    "\n",
    "        df_performance_train[task] = pd.DataFrame()\n",
    "        df_performance_val[task] = pd.DataFrame()\n",
    "\n",
    "        df_outputlabel_train[task] = pd.DataFrame()\n",
    "        df_outputlabel_val[task] = pd.DataFrame()\n",
    "\n",
    "        \n",
    "    main_task = training_params['output_names'][0].split('-')[0]\n",
    "    \n",
    "    N_CV = training_params['split_name_encoder'].classes_.shape[0] - 1\n",
    "    for i_CV in range(N_CV):\n",
    "        \n",
    "        if 'CV_max' in training_params:\n",
    "            if i_CV >= training_params['CV_max']:\n",
    "                continue\n",
    "\n",
    "        training_params['CV_config']['CV'] = i_CV\n",
    "\n",
    "        device = torch.device('cuda:{}'.format(int(training_params['cuda_i'])) if torch.cuda.is_available() else 'cpu')\n",
    "        print('using device', device)\n",
    "        print('using model ', training_params['model_name'])\n",
    "\n",
    "        model = PPG_compressor(training_params=training_params)\n",
    "        model = model.to(device).float()\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'], weight_decay=0.01)\n",
    "        criterion = VAELoss(training_params)\n",
    "        \n",
    "        training_params['criterion'] = criterion\n",
    "        training_params['optimizer'] = optimizer\n",
    "        training_params['inputdir'] = inputdir\n",
    "\n",
    "        CV_dict = train_model(model, training_params, trainer, evaler, preder)\n",
    "\n",
    "        plot_losses(CV_dict, outputdir=training_params['outputdir_sweep'], show_plot=False)\n",
    "\n",
    "        for task in training_params['model_out_names']:\n",
    "            if 'domain' in task:\n",
    "                continue\n",
    "        \n",
    "            label_est_val = CV_dict['performance_dict_val']['out_dict'][task]\n",
    "            label_val = CV_dict['performance_dict_val']['label_dict'][task]\n",
    "\n",
    "            label_est_train = CV_dict['performance_dict_train']['out_dict'][task]\n",
    "            label_train = CV_dict['performance_dict_train']['label_dict'][task]\n",
    "            \n",
    "            \n",
    "            mu_arr_train = CV_dict['performance_dict_train']['mu_arr']\n",
    "            logvar_arr_train = CV_dict['performance_dict_train']['logvar_arr']\n",
    "            \n",
    "            mu_arr_val = CV_dict['performance_dict_val']['mu_arr']\n",
    "            logvar_arr_val = CV_dict['performance_dict_val']['logvar_arr']\n",
    "            \n",
    "#             print(mu_arr_train.mean(axis=0))\n",
    "#             print(logvar_arr_train.mean(axis=0))\n",
    "#             print(mu_arr_val.mean(axis=0))\n",
    "#             print(logvar_arr_val.mean(axis=0))\n",
    "            \n",
    "#             sys.exit()\n",
    "            \n",
    "            data_saver(mu_arr_train, 'mu', training_params['outputdir_numeric']+'train/')\n",
    "            data_saver(logvar_arr_train, 'logvar', training_params['outputdir_numeric']+'train/')\n",
    "            \n",
    "            data_saver(mu_arr_val, 'mu', training_params['outputdir_numeric']+'val/')\n",
    "            data_saver(logvar_arr_val, 'logvar', training_params['outputdir_numeric']+'val/')\n",
    "\n",
    "            \n",
    "#             if 'domain' in task:\n",
    "#                 np.argmax(a, axis=1)\n",
    "            \n",
    "            \n",
    "#             # rescale the label after making estimations\n",
    "#             if 'perc' in training_params['output_names'][0]:\n",
    "#                 i_meta = training_params['meta_names'].index('EEavg_est')\n",
    "# #                 print(CV_dict['performance_dict_train']['meta_arr'], CV_dict['performance_dict_train']['meta_arr'].shape)\n",
    "#                 meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "#                 meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "#                 label_train = label_train*meta_train\n",
    "#                 label_val = label_val*meta_val\n",
    "#                 label_est_train = label_est_train*meta_train\n",
    "#                 label_est_val = label_est_val*meta_val\n",
    "#             elif 'weighted' in training_params['output_names'][0]:\n",
    "#                 i_meta = training_params['meta_names'].index('weight')\n",
    "#                 meta_train = CV_dict['performance_dict_train']['meta_arr'][:, i_meta]\n",
    "#                 meta_val = CV_dict['performance_dict_val']['meta_arr'][:, i_meta]\n",
    "\n",
    "#                 label_train = label_train*meta_train\n",
    "#                 label_val = label_val*meta_val\n",
    "#                 label_est_train = label_est_train*meta_train\n",
    "#                 label_est_val = label_est_val*meta_val\n",
    "\n",
    "                \n",
    "#             print(label_val, label_est_val)\n",
    "#             sys.exit()\n",
    "            \n",
    "            # get performance df for training and testing dataset\n",
    "#             df_performance_train[task] = df_performance_train[task].append( get_df_performance(label_train, label_est_train, i_CV, task), ignore_index=True )\n",
    "\n",
    "#             df_performance_train[task].to_csv(training_params['outputdir_numeric']  + 'df_performance_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "#             df_outputlabel_train[task] = df_outputlabel_train[task].append(\n",
    "#                 pd.DataFrame( {\n",
    "#                 'label_est': label_est_train,\n",
    "#                 'label': label_train,\n",
    "#                 'CV': [i_CV]*label_train.shape[0],\n",
    "#                 'task': [task]*label_train.shape[0]\n",
    "#                 }), ignore_index=True )\n",
    "\n",
    "#             df_outputlabel_train[task].to_csv(training_params['outputdir_numeric']  + 'df_outputlabel_train_{}.csv'.format(task), index=False)\n",
    "\n",
    "            # df_performance_val[task] = df_performance_val[task].append( get_df_performance(label_val, label_est_val, i_CV, task), ignore_index=True )\n",
    "            # df_performance_val[task].to_csv(training_params['outputdir_numeric']  + 'df_performance_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "#             df_outputlabel_val[task] = df_outputlabel_val[task].append(\n",
    "#                 pd.DataFrame( {\n",
    "#                 'label_est': label_est_val,\n",
    "#                 'label': label_val,\n",
    "#                 'CV': [i_CV]*label_val.shape[0],\n",
    "#                 'task': [task]*label_val.shape[0]\n",
    "#                 }), ignore_index=True )\n",
    "\n",
    "#             df_outputlabel_val[task].to_csv(training_params['outputdir_numeric']  + 'df_outputlabel_val_{}.csv'.format(task), index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # plot performance training and testing dataset\n",
    "            if (main_task not in task) and (debug_auxillary==False):\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            plot_one_window(model, training_params, mode='train', fig_name='signals_CV{}'.format(i_CV), outputdir=training_params['outputdir_modelout']+'train/', show_plot=False)\n",
    "\n",
    "            plot_one_window(model, training_params, mode='val', fig_name='signals_CV{}'.format(i_CV), outputdir=training_params['outputdir_modelout']+'val/', show_plot=False)\n",
    "\n",
    "            # sys.exit()\n",
    "\n",
    "            # plot_conf(df_outputlabel_train[task], training_params,  fig_name='cm_train', show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "            # plot_conf(df_outputlabel_val[task], training_params,  fig_name='cm_val', show_plot=False, outputdir=training_params['outputdir_modelout'])\n",
    "            # sys.exit()\n",
    "\n",
    "            # plot_regression(df_outputlabel_train[task], df_performance_train[task], task, fig_name='regression_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "#             plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "            # plot_regression(df_outputlabel_val[task], df_performance_val[task], task, fig_name='regression_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "#             plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir+'model_output/')\n",
    "\n",
    "#             plot_output(df_outputlabel_train[task], task, fig_name = 'outputINtime_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout)\n",
    "        \n",
    "        # check_featuremap(model, training_params, mode='worst', fig_name = 'DL_activation_{}_'.format(i_CV), outputdir=outputdir+'activation_layers_worst/{}/'.format(i_CV), show_plot=False)\n",
    "        # check_featuremap(model, training_params, mode='best', fig_name = 'DL_activation_{}_'.format(i_CV), outputdir=outputdir+'activation_layers_best/{}/'.format(i_CV), show_plot=False)\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    # for task in training_params['model_out_names']:\n",
    "    #     if main_task not in task:\n",
    "    #         continue\n",
    "#         if task!=main_task:\n",
    "#             continue\n",
    "#         plot_regression_all_agg(df_outputlabel_train[task], df_performance_train[task], fig_name='LinearR_agg_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "#         plot_BA(df_outputlabel_train[task], task, fig_name='BA_train_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "\n",
    "#         plot_regression_all_agg(df_outputlabel_val[task], df_performance_val[task], fig_name='LinearR_agg_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "#         plot_BA(df_outputlabel_val[task], task, fig_name='BA_val_{}'.format(task), show_plot=False, outputdir=outputdir_modelout, log_wandb=training_params['wandb'])\n",
    "\n",
    "        # plot_output(df_outputlabel_val[task], task, fig_name = 'outputINtime_val_{}'.format(task),  show_plot=False, outputdir=outputdir_modelout)\n",
    "\n",
    "#     plot_BA(df_outputlabel_val[main_task], main_task, fig_name='BA_val_{}'.format(main_task), show_plot=False, outputdir=outputdir+'model_output/', log_wandb=training_params['wandb'])\n",
    "#     plot_regression_all_agg(df_outputlabel_val[main_task], df_performance_val[main_task], outputdir=outputdir+'model_output/', show_plot=False, log_wandb=training_params['wandb'])\n",
    "\n",
    "    # log metrices on wnadb\n",
    "    if training_params['wandb']==True:\n",
    "        \n",
    "#         label = df_outputlabel_val[main_task]['label'].values\n",
    "#         label_est = df_outputlabel_val[main_task]['label_est'].values\n",
    "# #         print(label.shape, label)\n",
    "# #         print(label_est.shape, label_est)\n",
    "    \n",
    "#         PCC = get_PCC(label, label_est)\n",
    "#         Rsquared = get_CoeffDeterm(label, label_est)\n",
    "#         MAE, _ = get_MAE(label, label_est)\n",
    "#         RMSE = get_RMSE(label, label_est)\n",
    "#         MAPE, _ = get_MAPE(label, label_est)\n",
    "\n",
    "        \n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                'val_total_loss': CV_dict['df_losses_val']['total'].values[-1],\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_sweep is a function that wandb calls when changing to a new HP set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep(config=None):\n",
    "\n",
    "    with wandb.init(config=config, reinit=True, dir=outputdir):\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        print(config)\n",
    "        \n",
    "        # init the model\n",
    "        for key in config.keys():\n",
    "            if key=='loss_weights':\n",
    "                training_params[key]['auxillary_task'] = config[key]\n",
    "            else:\n",
    "                training_params[key] = config[key]\n",
    "\n",
    "        train_master(training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# master wandb that select the HP set and ask the train_sweep to train and eval a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time: 2022-Jun-28 22:56:27\n"
     ]
    }
   ],
   "source": [
    "tz_NY = pytz.timezone('America/New_York') \n",
    "datetime_start = datetime.now(tz_NY)\n",
    "print(\"start time:\", datetime_start.strftime(\"%Y-%b-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:1\n",
      "using model  PPG_VAEcompressor\n",
      "feature_out_dim : 1792\n",
      "\t start training.....\n",
      "\t[1st epoch]\n",
      "\t[11th epoch]\n",
      "\t[21st epoch]\n",
      "\t[31st epoch]\n",
      "\t[41st epoch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-25622a695175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msweep_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sweep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_master\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-141532d82ccb>\u001b[0m in \u001b[0;36mtrain_master\u001b[0;34m(training_params)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtraining_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputdir'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mCV_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCV_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'outputdir_sweep'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/disparities_O2/repo/PhysioMC/DR_extension/training_util.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, training_params, trainer, evaler, preder)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;31m##### model training mode ####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         \u001b[0mperformance_dict_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;31m# df_losses_train = df_losses_train.append(  pd.DataFrame(performance_dict_train, index=[0]), ignore_index=True )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/disparities_O2/repo/PhysioMC/DR_extension/training_util.py\u001b[0m in \u001b[0;36mtrain_dann\u001b[0;34m(model, dataloader, training_params)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mienv/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mienv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mienv/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mienv/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if training_params['wandb']:\n",
    "    print('sweeping for:', sweep_name)\n",
    "    # get the config of current sweep\n",
    "    sweep_config = training_params['sweep_config']    \n",
    "    # get the config of current sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, entity='inanlab', project='[PPG_compress] stage3_'+training_params['sweep_name'])\n",
    "    wandb.agent(sweep_id, train_sweep)\n",
    "else:\n",
    "    train_master(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel_size = 10\n",
    "# # m = nn.Conv1d(16, 33, kernel_size=kernel_size, stride=1)\n",
    "# input = torch.randn(20, 16, 1)\n",
    "# # output = m(input)\n",
    "# print(input.size())\n",
    "# # print\n",
    "# m = torch.nn.ConvTranspose1d(16, 33, kernel_size, stride=1,)\n",
    "# output = m(input)\n",
    "# print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# num_of_gpus = torch.cuda.device_count()\n",
    "# print(num_of_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datetime_end = datetime.now(tz_NY)\n",
    "print(\"end time:\", datetime_end.strftime(\"%Y-%b-%d %H:%M:%S\"))\n",
    "\n",
    "duration = datetime_end-datetime_start\n",
    "duration_in_s = duration.total_seconds()\n",
    "days    = divmod(duration_in_s, 86400)        # Get days (without [0]!)\n",
    "hours   = divmod(days[1], 3600)               # Use remainder of days to calc hours\n",
    "minutes = divmod(hours[1], 60)                # Use remainder of hours to calc minutes\n",
    "seconds = divmod(minutes[1], 1)               # Use remainder of minutes to calc seconds\n",
    "print(\"Time between dates: %d days, %d hours, %d minutes and %d seconds\" % (days[0], hours[0], minutes[0], seconds[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wrap it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if training_params['wandb']:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
